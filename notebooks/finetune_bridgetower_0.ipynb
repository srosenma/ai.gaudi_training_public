{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HLS_MODULE_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95177612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from transformers.models.bridgetower.modeling_bridgetower import BridgeTowerForContrastiveLearning\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoImageProcessor\n",
    "import torch\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode, to_grayscale, to_tensor\n",
    "\n",
    "from optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f6f85",
   "metadata": {},
   "source": [
    "# Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89da95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dset = load_dataset(\"jmhessel/newyorker_caption_contest\", \"matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training dataset\n",
    "train_dataset = dset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 213\n",
    "k = 199\n",
    "print(dset[\"train\"][k]['image_description'])\n",
    "dset[\"train\"][k]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0000a083",
   "metadata": {},
   "source": [
    "# Load pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name_or_path = \"BridgeTower/bridgetower-large-itm-mlm-itc\"\n",
    "model = BridgeTowerForContrastiveLearning.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01e86e",
   "metadata": {},
   "source": [
    "# Tokenize the train dataset captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions using the tokenizer\n",
    "def tokenize_captions(examples):\n",
    "        captions = list(examples[\"image_description\"])\n",
    "        text_inputs = tokenizer(captions, max_length=128, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c411a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions of all the train dataset\n",
    "train_dataset = train_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dset[\"train\"].column_names if col != \"image\"],\n",
    "            num_proc=None,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a2be4",
   "metadata": {},
   "source": [
    "# Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to grayscale and tensor\n",
    "def get_image(image_or_path):\n",
    "    image_or_path = to_grayscale(image_or_path, num_output_channels=3)\n",
    "    return to_tensor(image_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess of the image: Resize, CenterCrop, ConvertImageDtype and Normalize\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81addaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image_processor, in this script we only use this to get the mean and std for normalization.\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n",
    "image_size = model.config.vision_config.image_size\n",
    "\n",
    "image_transformations = Transform(image_size, image_processor.image_mean, image_processor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7166555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply image transformations to the images in the examples\n",
    "def transform_images(examples):\n",
    "    images = [get_image(image_file) for image_file in examples[\"image\"]]\n",
    "    examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transform (image processor) is applied on-the-fly on batches when __getitem__ is called\n",
    "train_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772edb8",
   "metadata": {},
   "source": [
    "# Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to use to form a batch from a list of elements of train_dataset\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d787c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "\n",
    "num_train_epochs = 1\n",
    "per_device_train_batch_size = 8\n",
    "use_lazy_mode = True\n",
    "learning_rate = 5e-05\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"test_trainer\",\n",
    "#     remove_unused_columns=False,\n",
    "#     num_train_epochs=1,\n",
    "#     report_to=[],\n",
    "#     logging_steps=50\n",
    "# )\n",
    "\n",
    "training_args = GaudiTrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    remove_unused_columns=False,\n",
    "    use_habana=True,\n",
    "    use_lazy_mode=use_lazy_mode,\n",
    "    use_hpu_graphs_for_inference=True,\n",
    "    gaudi_config_name=\"Habana/clip\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    report_to=[],\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77794bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaudi_config = GaudiConfig.from_pretrained(training_args.gaudi_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize the trainer\n",
    "\n",
    "# trainer = Trainer(\n",
    "#         model=model,\n",
    "#         #gaudi_config=gaudi_config,\n",
    "#         #args=training_args,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         data_collator=collate_fn,\n",
    "#     )\n",
    "\n",
    "trainer = GaudiTrainer(\n",
    "    model=model,\n",
    "    gaudi_config=gaudi_config,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa673f60",
   "metadata": {},
   "source": [
    "# Train the model ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0baf08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_result = trainer.train(resume_from_checkpoint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e2973",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd237b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDX_EXAMPLE_1 = 0\n",
    "print(dset[\"train\"][INDX_EXAMPLE_1]['image_description'])\n",
    "dset[\"train\"][INDX_EXAMPLE_1]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDX_EXAMPLE_2 = 5\n",
    "print(dset[\"train\"][INDX_EXAMPLE_2]['image_description'])\n",
    "dset[\"train\"][INDX_EXAMPLE_2]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to HPU\n",
    "device = torch.device(\"hpu\")\n",
    "\n",
    "ex_1 = train_dataset[INDX_EXAMPLE_1]  # Get example 1 from the train dataset\n",
    "ex_2 = train_dataset[INDX_EXAMPLE_2]  # Get example 2 from the train dataset\n",
    "\n",
    "\n",
    "# Extract pixel values, input IDs, and attention mask from example 1\n",
    "pixel_values_1 = torch.stack([ex_1['pixel_values']]).to(device)\n",
    "input_ids_1 = torch.tensor([ex_1['input_ids']]).to(device)\n",
    "attention_mask_1 = torch.tensor([ex_1['attention_mask']]).to(device)\n",
    "\n",
    "\n",
    "# Extract pixel values, input IDs, and attention mask from example 2\n",
    "pixel_values_2 = torch.stack([ex_2['pixel_values']]).to(device)\n",
    "input_ids_2 = torch.tensor([ex_2['input_ids']]).to(device)\n",
    "attention_mask_2 = torch.tensor([ex_2['attention_mask']]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Create an encoding dictionary for example 1\n",
    "encoding_1 = {\n",
    "    \"pixel_values\": pixel_values_2,\n",
    "    \"input_ids\": input_ids_1,\n",
    "    \"attention_mask\": attention_mask_1,\n",
    "}\n",
    "\n",
    "# Pass the example to the fine-thuned model\n",
    "outputs_1 = trainer.model(**encoding_1)\n",
    "logits_text_to_image_1 = torch.matmul(outputs_1['text_embeds'], outputs_1['image_embeds'].t()).to('cpu')\n",
    "print(f\"logits_text_to_image_1: {logits_text_to_image_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53f30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b0cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the example to the baesline model\n",
    "\n",
    "outputs_1 = model(**encoding_1)\n",
    "logits_text_to_image_1 = torch.matmul(outputs_1['text_embeds'], outputs_1['image_embeds'].t()) \n",
    "print(f\"logits_text_to_image_1: {logits_text_to_image_1.to('cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90e7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893e467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
